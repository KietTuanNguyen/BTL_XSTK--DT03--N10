confusion_matrix <- table(Y_test, predictions)
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
# In kết quả
print(confusion_matrix)
print(paste("Accuracy:", round(accuracy, 4)))
# Xem độ quan trọng của các biến
importance_matrix <- xgb.importance(model = model)
print(importance_matrix)
xgb.plot.importance(importance_matrix)
library(xgboost)
set.seed(100)
# Chuyển đổi dữ liệu thành định dạng DMatrix của XGBoost
dtrain <- xgb.DMatrix(data = as.matrix(X_train), label = as.numeric(Y_train) - 1)
dtest <- xgb.DMatrix(data = as.matrix(X_test), label = as.numeric(Y_test) - 1)
# Đặt tham số cho mô hình XGBoost
params <- list(
objective = "multi:softprob",
num_class = length(unique(Y_train)),
eval_metric = "mlogloss",
eta = 0.05,  # learning rate
max_depth = 6,
subsample = 0.8,
colsample_bytree = 0.8
)
# Số vòng lặp (trees)
nrounds <- 100
# Huấn luyện mô hình
model <- xgb.train(
params = params,
data = dtrain,
nrounds = nrounds,
watchlist = list(train = dtrain, eval = dtest),
print_every_n = 10
)
# Dự đoán trên tập dữ liệu kiểm tra
pred_probs <- predict(model, as.matrix(X_test))
predictions <- max.col(matrix(pred_probs, ncol = length(unique(Y_train)), byrow = TRUE)) - 1
# Tạo bảng confusion matrix và tính độ chính xác
confusion_matrix <- table(Y_test, predictions)
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
# In kết quả
print(confusion_matrix)
print(paste("Accuracy:", round(accuracy, 4)))
# Xem độ quan trọng của các biến
importance_matrix <- xgb.importance(model = model)
print(importance_matrix)
xgb.plot.importance(importance_matrix)
library(xgboost)
set.seed(100)
# Chuyển đổi dữ liệu thành định dạng DMatrix của XGBoost
dtrain <- xgb.DMatrix(data = as.matrix(X_train), label = as.numeric(Y_train) - 1)
dtest <- xgb.DMatrix(data = as.matrix(X_test), label = as.numeric(Y_test) - 1)
# Đặt tham số cho mô hình XGBoost
params <- list(
objective = "multi:softprob",
num_class = length(unique(Y_train)),
eval_metric = "mlogloss",
eta = 0.01,  # learning rate
max_depth = 6,
subsample = 0.8,
colsample_bytree = 0.8
)
# Số vòng lặp (trees)
nrounds <- 100
# Huấn luyện mô hình
model <- xgb.train(
params = params,
data = dtrain,
nrounds = nrounds,
watchlist = list(train = dtrain, eval = dtest),
print_every_n = 10
)
# Dự đoán trên tập dữ liệu kiểm tra
pred_probs <- predict(model, as.matrix(X_test))
predictions <- max.col(matrix(pred_probs, ncol = length(unique(Y_train)), byrow = TRUE)) - 1
# Tạo bảng confusion matrix và tính độ chính xác
confusion_matrix <- table(Y_test, predictions)
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
# In kết quả
print(confusion_matrix)
print(paste("Accuracy:", round(accuracy, 4)))
# Xem độ quan trọng của các biến
importance_matrix <- xgb.importance(model = model)
print(importance_matrix)
xgb.plot.importance(importance_matrix)
library(xgboost)
set.seed(100)
# Chuyển đổi dữ liệu thành định dạng DMatrix của XGBoost
dtrain <- xgb.DMatrix(data = as.matrix(X_train), label = as.numeric(Y_train) - 1)
dtest <- xgb.DMatrix(data = as.matrix(X_test), label = as.numeric(Y_test) - 1)
# Đặt tham số cho mô hình XGBoost
params <- list(
objective = "multi:softprob",
num_class = length(unique(Y_train)),
eval_metric = "mlogloss",
eta = 0.1,  # learning rate
max_depth = 6,
subsample = 0.8,
colsample_bytree = 0.8
)
# Số vòng lặp (trees)
nrounds <- 100
# Huấn luyện mô hình
model <- xgb.train(
params = params,
data = dtrain,
nrounds = nrounds,
watchlist = list(train = dtrain, eval = dtest),
print_every_n = 10
)
# Dự đoán trên tập dữ liệu kiểm tra
pred_probs <- predict(model, as.matrix(X_test))
predictions <- max.col(matrix(pred_probs, ncol = length(unique(Y_train)), byrow = TRUE)) - 1
# Tạo bảng confusion matrix và tính độ chính xác
confusion_matrix <- table(Y_test, predictions)
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
# In kết quả
print(confusion_matrix)
print(paste("Accuracy:", round(accuracy, 4)))
# Xem độ quan trọng của các biến
importance_matrix <- xgb.importance(model = model)
print(importance_matrix)
xgb.plot.importance(importance_matrix)
library(xgboost)
set.seed(100)
# Chuyển đổi dữ liệu thành định dạng DMatrix của XGBoost
dtrain <- xgb.DMatrix(data = as.matrix(X_train), label = as.numeric(Y_train) - 1)
dtest <- xgb.DMatrix(data = as.matrix(X_test), label = as.numeric(Y_test) - 1)
# Đặt tham số cho mô hình XGBoost
params <- list(
objective = "multi:softprob",
num_class = length(unique(Y_train)),
eval_metric = "mlogloss",
eta = 0.1,  # learning rate
max_depth = 6,
subsample = 0.7,
colsample_bytree = 0.7
)
# Số vòng lặp (trees)
nrounds <- 100
# Huấn luyện mô hình
model <- xgb.train(
params = params,
data = dtrain,
nrounds = nrounds,
watchlist = list(train = dtrain, eval = dtest),
print_every_n = 10
)
# Dự đoán trên tập dữ liệu kiểm tra
pred_probs <- predict(model, as.matrix(X_test))
predictions <- max.col(matrix(pred_probs, ncol = length(unique(Y_train)), byrow = TRUE)) - 1
# Tạo bảng confusion matrix và tính độ chính xác
confusion_matrix <- table(Y_test, predictions)
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
# In kết quả
print(confusion_matrix)
print(paste("Accuracy:", round(accuracy, 4)))
# Xem độ quan trọng của các biến
importance_matrix <- xgb.importance(model = model)
print(importance_matrix)
xgb.plot.importance(importance_matrix)
# Read file CSV
X_train <- read.csv("../Data/X_train.csv")
Y_train <- read.csv("../Data/Y_train.csv", header = TRUE)$Condition  # Ensure Y_train is read as a vector
X_test <- read.csv("../Data/X_test.csv")
Y_test <- read.csv("../Data/Y_test.csv", header = TRUE)$Condition  # Ensure Y_test is read as a vector
# Convert Y_train as_factor
if (!is.factor(Y_train)) {
Y_train <- as.factor(Y_train)
}
# Load torch library
library(torch)
library(torchvision)
if(cuda_is_available()){
device <- torch_device('cuda')
} else {
device <- torch_device('cpu')
}
# Convert data to tensors
X_train <- torch_tensor(as.matrix(X_train), dtype = torch_float())
X_test <- torch_tensor(as.matrix(X_test), dtype = torch_float())
Y_train <- torch_tensor(as.numeric(Y_train), dtype = torch_long())  # Adjust labels if needed
Y_test <- torch_tensor(as.numeric(Y_test), dtype = torch_long())    # Adjust labels if needed
# Define the MLP model with hidden layers
model <- nn_sequential(
nn_linear(in_features = ncol(X_train), out_features = 64),
nn_relu(),
nn_dropout(p=0.5),
nn_linear(in_features = 64, out_features = 32),
nn_relu(),
nn_dropout(p=0.5),
nn_linear(in_features = 32, out_features = 32),
nn_relu(),
nn_dropout(p=0.5),
nn_linear(in_features = 32, out_features = 16),
nn_relu(),
nn_dropout(p=0.5),
nn_linear(in_features = 16 ,out_features = 3),
)
# Define loss function and optimizer
criterion <- nn_cross_entropy_loss()
optimizer <- optim_adam(model$parameters, lr = 0.001)
num_epochs <- 1000
# Initialize vectors to store accuracies
accuracys_test <- c()
accuracys_train <- c()
# Initialize vectors to store loss history
loss_history_train <- c()
loss_history_test <- c()
for (epoch in 0:num_epochs) {
# Train for one epoch
optimizer$zero_grad()
Y_pred_train <- model(X_train)
Y_pred_test <- model(X_test)
loss_train <- criterion(Y_pred_train, Y_train)
loss_test <- criterion(Y_pred_test, Y_test)  # Calculate loss for test set
loss_train$backward()
optimizer$step()
# Append loss to loss_history vectors
loss_history_train <- c(loss_history_train, loss_train$item())
loss_history_test <- c(loss_history_test, loss_test$item())
if (epoch %% 5 == 0) {
predicted_train <- Y_pred_train$argmax(dim = 2)
accuracy_train <- (predicted_train == Y_train)$sum()$item() / Y_train$size()
predicted_labels <- Y_pred_test$argmax(dim = 2)
accuracy_test <- (predicted_labels == Y_test)$sum()$item() / Y_test$size()
cat("Epoch:", epoch, " Loss_Train:", loss_train$item(), " Loss_Test:", loss_test$item(), " Accuracy_Train: ", accuracy_train, " Accuracy_Test: ", accuracy_test, "\n")
# Append accuracies to respective vectors
accuracys_train <- c(accuracys_train, accuracy_train)
accuracys_test <- c(accuracys_test, accuracy_test)
}
}
# Evaluate the model
model$eval()
# Load the ggplot2 library
library(ggplot2)
# Create a dataframe to store the loss and accuracies
plot_data_loss <- data.frame(
epoch = seq(0, num_epochs, by = 5),
loss_train = rep(0, length.out = length(seq(0, num_epochs, by = 5))),
loss_test = rep(0, length.out = length(seq(0, num_epochs, by = 5)))
)
plot_data_accuracy <- data.frame(
epoch = seq(0, num_epochs, by = 5),
accuracy_train = rep(0, length.out = length(seq(0, num_epochs, by = 5))),
accuracy_test = rep(0, length.out = length(seq(0, num_epochs, by = 5)))
)
# Update plot_data_loss with loss values
for (i in 1:nrow(plot_data_loss)) {
plot_data_loss[i, "loss_train"] <- loss_history_train[i]  # Assuming loss_history_train is defined
plot_data_loss[i, "loss_test"] <- loss_history_test[i]  # Assuming loss_history_test is defined
}
# Update plot_data_accuracy with accuracy values
for (i in 1:nrow(plot_data_accuracy)) {
plot_data_accuracy[i, "accuracy_train"] <- accuracys_train[i]  # Assuming accuracys_train is defined
plot_data_accuracy[i, "accuracy_test"] <- accuracys_test[i]  # Assuming accuracys_test is defined
}
# Create the plot for loss
plot_loss <- ggplot(plot_data_loss, aes(x = epoch)) +
geom_line(aes(y = loss_train, color = "Training Loss")) +
geom_line(aes(y = loss_test, color = "Test Loss")) +
labs(title = "Loss Over Epochs",
x = "Epoch",
y = "Loss") +
scale_color_manual(values = c("Training Loss" = "blue", "Test Loss" = "red")) +
theme_minimal()
# Create the plot for accuracies
plot_accuracy <- ggplot(plot_data_accuracy, aes(x = epoch)) +
geom_line(aes(y = accuracy_train, color = "Training Accuracy")) +
geom_line(aes(y = accuracy_test, color = "Test Accuracy")) +
labs(title = "Accuracies Over Epochs",
x = "Epoch",
y = "Accuracy") +
scale_color_manual(values = c("Training Accuracy" = "green", "Test Accuracy" = "purple")) +
theme_minimal()
# Display the plots
print(plot_loss)
print(plot_accuracy)
# Read file CSV
X_train <- read.csv("../Data/X_train.csv")
Y_train <- read.csv("../Data/Y_train.csv", header = TRUE)$Condition  # Ensure Y_train is read as a vector
X_test <- read.csv("../Data/X_test.csv")
Y_test <- read.csv("../Data/Y_test.csv", header = TRUE)$Condition  # Ensure Y_test is read as a vector
# Convert Y_train as_factor
if (!is.factor(Y_train)) {
Y_train <- as.factor(Y_train)
}
# Load torch library
library(torch)
library(torchvision)
if(cuda_is_available()){
device <- torch_device('cuda')
} else {
device <- torch_device('cpu')
}
# Convert data to tensors
X_train <- torch_tensor(as.matrix(X_train), dtype = torch_float())
X_test <- torch_tensor(as.matrix(X_test), dtype = torch_float())
Y_train <- torch_tensor(as.numeric(Y_train), dtype = torch_long())  # Adjust labels if needed
Y_test <- torch_tensor(as.numeric(Y_test), dtype = torch_long())    # Adjust labels if needed
# Define the MLP model with hidden layers
model <- nn_sequential(
nn_linear(in_features = ncol(X_train), out_features = 64),
nn_relu(),
nn_dropout(p=0.5),
nn_linear(in_features = 64, out_features = 32),
nn_relu(),
nn_dropout(p=0.5),
nn_linear(in_features = 32, out_features = 32),
nn_relu(),
nn_dropout(p=0.5),
nn_linear(in_features = 32, out_features = 16),
nn_relu(),
nn_dropout(p=0.5),
nn_linear(in_features = 16 ,out_features = 3),
)
# Define loss function and optimizer
criterion <- nn_cross_entropy_loss()
optimizer <- optim_adam(model$parameters, lr = 0.001)
num_epochs <- 1000
# Initialize vectors to store accuracies
accuracys_test <- c()
accuracys_train <- c()
# Initialize vectors to store loss history
loss_history_train <- c()
loss_history_test <- c()
for (epoch in 0:num_epochs) {
# Train for one epoch
optimizer$zero_grad()
Y_pred_train <- model(X_train)
Y_pred_test <- model(X_test)
loss_train <- criterion(Y_pred_train, Y_train)
loss_test <- criterion(Y_pred_test, Y_test)  # Calculate loss for test set
loss_train$backward()
optimizer$step()
# Append loss to loss_history vectors
loss_history_train <- c(loss_history_train, loss_train$item())
loss_history_test <- c(loss_history_test, loss_test$item())
if (epoch %% 5 == 0) {
predicted_train <- Y_pred_train$argmax(dim = 2)
accuracy_train <- (predicted_train == Y_train)$sum()$item() / Y_train$size()
predicted_labels <- Y_pred_test$argmax(dim = 2)
accuracy_test <- (predicted_labels == Y_test)$sum()$item() / Y_test$size()
cat("Epoch:", epoch, " Loss_Train:", loss_train$item(), " Loss_Test:", loss_test$item(), " Accuracy_Train: ", accuracy_train, " Accuracy_Test: ", accuracy_test, "\n")
# Append accuracies to respective vectors
accuracys_train <- c(accuracys_train, accuracy_train)
accuracys_test <- c(accuracys_test, accuracy_test)
}
}
# Evaluate the model
model$eval()
# Load the ggplot2 library
library(ggplot2)
# Create a dataframe to store the loss and accuracies
plot_data_loss <- data.frame(
epoch = seq(0, num_epochs, by = 5),
loss_train = rep(0, length.out = length(seq(0, num_epochs, by = 5))),
loss_test = rep(0, length.out = length(seq(0, num_epochs, by = 5)))
)
plot_data_accuracy <- data.frame(
epoch = seq(0, num_epochs, by = 5),
accuracy_train = rep(0, length.out = length(seq(0, num_epochs, by = 5))),
accuracy_test = rep(0, length.out = length(seq(0, num_epochs, by = 5)))
)
# Update plot_data_loss with loss values
for (i in 1:nrow(plot_data_loss)) {
plot_data_loss[i, "loss_train"] <- loss_history_train[i]  # Assuming loss_history_train is defined
plot_data_loss[i, "loss_test"] <- loss_history_test[i]  # Assuming loss_history_test is defined
}
# Update plot_data_accuracy with accuracy values
for (i in 1:nrow(plot_data_accuracy)) {
plot_data_accuracy[i, "accuracy_train"] <- accuracys_train[i]  # Assuming accuracys_train is defined
plot_data_accuracy[i, "accuracy_test"] <- accuracys_test[i]  # Assuming accuracys_test is defined
}
# Create the plot for loss
plot_loss <- ggplot(plot_data_loss, aes(x = epoch)) +
geom_line(aes(y = loss_train, color = "Training Loss")) +
geom_line(aes(y = loss_test, color = "Test Loss")) +
labs(title = "Loss Over Epochs",
x = "Epoch",
y = "Loss") +
scale_color_manual(values = c("Training Loss" = "blue", "Test Loss" = "red")) +
theme_minimal()
# Create the plot for accuracies
plot_accuracy <- ggplot(plot_data_accuracy, aes(x = epoch)) +
geom_line(aes(y = accuracy_train, color = "Training Accuracy")) +
geom_line(aes(y = accuracy_test, color = "Test Accuracy")) +
labs(title = "Accuracies Over Epochs",
x = "Epoch",
y = "Accuracy") +
scale_color_manual(values = c("Training Accuracy" = "green", "Test Accuracy" = "purple")) +
theme_minimal()
# Display the plots
print(plot_loss)
print(plot_accuracy)
# Read file CSV
X_train <- read.csv("../Data/X_train.csv")
Y_train <- read.csv("../Data/Y_train.csv", header = TRUE)$Condition  # Ensure Y_train is read as a vector
X_test <- read.csv("../Data/X_test.csv")
Y_test <- read.csv("../Data/Y_test.csv", header = TRUE)$Condition  # Ensure Y_test is read as a vector
# Convert Y_train as_factor
if (!is.factor(Y_train)) {
Y_train <- as.factor(Y_train)
}
# Load torch library
library(torch)
library(torchvision)
if(cuda_is_available()){
device <- torch_device('cuda')
} else {
device <- torch_device('cpu')
}
# Convert data to tensors
X_train <- torch_tensor(as.matrix(X_train), dtype = torch_float())
X_test <- torch_tensor(as.matrix(X_test), dtype = torch_float())
Y_train <- torch_tensor(as.numeric(Y_train), dtype = torch_long())  # Adjust labels if needed
Y_test <- torch_tensor(as.numeric(Y_test), dtype = torch_long())    # Adjust labels if needed
# Define the MLP model with hidden layers
model <- nn_sequential(
nn_linear(in_features = ncol(X_train), out_features = 64),
nn_relu(),
nn_dropout(p=0.5),
nn_linear(in_features = 64, out_features = 32),
nn_relu(),
nn_dropout(p=0.5),
nn_linear(in_features = 32, out_features = 32),
nn_relu(),
nn_dropout(p=0.5),
nn_linear(in_features = 32, out_features = 16),
nn_relu(),
nn_dropout(p=0.5),
nn_linear(in_features = 16 ,out_features = 3),
)
# Define loss function and optimizer
criterion <- nn_cross_entropy_loss()
optimizer <- optim_adam(model$parameters, lr = 0.001)
num_epochs <- 1000
# Initialize vectors to store accuracies
accuracys_test <- c()
accuracys_train <- c()
# Initialize vectors to store loss history
loss_history_train <- c()
loss_history_test <- c()
for (epoch in 0:num_epochs) {
# Train for one epoch
optimizer$zero_grad()
Y_pred_train <- model(X_train)
Y_pred_test <- model(X_test)
loss_train <- criterion(Y_pred_train, Y_train)
loss_test <- criterion(Y_pred_test, Y_test)  # Calculate loss for test set
loss_train$backward()
optimizer$step()
# Append loss to loss_history vectors
loss_history_train <- c(loss_history_train, loss_train$item())
loss_history_test <- c(loss_history_test, loss_test$item())
if (epoch %% 5 == 0) {
predicted_train <- Y_pred_train$argmax(dim = 2)
accuracy_train <- (predicted_train == Y_train)$sum()$item() / Y_train$size()
predicted_labels <- Y_pred_test$argmax(dim = 2)
accuracy_test <- (predicted_labels == Y_test)$sum()$item() / Y_test$size()
cat("Epoch:", epoch, " Loss_Train:", loss_train$item(), " Loss_Test:", loss_test$item(), " Accuracy_Train: ", accuracy_train, " Accuracy_Test: ", accuracy_test, "\n")
# Append accuracies to respective vectors
accuracys_train <- c(accuracys_train, accuracy_train)
accuracys_test <- c(accuracys_test, accuracy_test)
}
}
# Evaluate the model
model$eval()
# Load the ggplot2 library
library(ggplot2)
# Create a dataframe to store the loss and accuracies
plot_data_loss <- data.frame(
epoch = seq(0, num_epochs, by = 5),
loss_train = rep(0, length.out = length(seq(0, num_epochs, by = 5))),
loss_test = rep(0, length.out = length(seq(0, num_epochs, by = 5)))
)
plot_data_accuracy <- data.frame(
epoch = seq(0, num_epochs, by = 5),
accuracy_train = rep(0, length.out = length(seq(0, num_epochs, by = 5))),
accuracy_test = rep(0, length.out = length(seq(0, num_epochs, by = 5)))
)
# Update plot_data_loss with loss values
for (i in 1:nrow(plot_data_loss)) {
plot_data_loss[i, "loss_train"] <- loss_history_train[i]  # Assuming loss_history_train is defined
plot_data_loss[i, "loss_test"] <- loss_history_test[i]  # Assuming loss_history_test is defined
}
# Update plot_data_accuracy with accuracy values
for (i in 1:nrow(plot_data_accuracy)) {
plot_data_accuracy[i, "accuracy_train"] <- accuracys_train[i]  # Assuming accuracys_train is defined
plot_data_accuracy[i, "accuracy_test"] <- accuracys_test[i]  # Assuming accuracys_test is defined
}
# Create the plot for loss
plot_loss <- ggplot(plot_data_loss, aes(x = epoch)) +
geom_line(aes(y = loss_train, color = "Training Loss")) +
geom_line(aes(y = loss_test, color = "Test Loss")) +
labs(title = "Loss Over Epochs",
x = "Epoch",
y = "Loss") +
scale_color_manual(values = c("Training Loss" = "blue", "Test Loss" = "red")) +
theme_minimal()
# Create the plot for accuracies
plot_accuracy <- ggplot(plot_data_accuracy, aes(x = epoch)) +
geom_line(aes(y = accuracy_train, color = "Training Accuracy")) +
geom_line(aes(y = accuracy_test, color = "Test Accuracy")) +
labs(title = "Accuracies Over Epochs",
x = "Epoch",
y = "Accuracy") +
scale_color_manual(values = c("Training Accuracy" = "green", "Test Accuracy" = "purple")) +
theme_minimal()
# Display the plots
print(plot_loss)
print(plot_accuracy)
