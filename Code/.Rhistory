labels = c(0, 1, 2)))
# Print the new encoded values
print(unique(bridge_data_subset$Condition))
summary(bridge_data_subset)
# Create boxplot and summary for Age
boxplot(Age ~ Condition, data = bridge_data_subset,
main = "Boxplot of Age Grouped by Condition",
xlab = "Condition", ylab = "Age")
summary_data_Age <- by(bridge_data_subset$Age, bridge_data_subset$Condition, summary)
print(summary_data_Age)
# Create boxplot and summary for Traffic
boxplot(Traffic ~ Condition, data = bridge_data_subset,
main = "Boxplot of Traffic Grouped by Condition",
xlab = "Condition", ylab = "Traffic")
summary_data_Traffic <- by(bridge_data_subset$Traffic, bridge_data_subset$Condition, summary)
print(summary_data_Traffic)
# Create boxplot and summary for Spans
boxplot(Spans ~ Condition, data = bridge_data_subset,
main = "Boxplot of Spans Grouped by Condition",
xlab = "Condition", ylab = "Spans")
summary_data_Spans <- by(bridge_data_subset$Spans, bridge_data_subset$Condition, summary)
print(summary_data_Spans)
# Create boxplot and summary for Length
boxplot(Length ~ Condition, data = bridge_data_subset,
main = "Boxplot of Length Grouped by Condition",
xlab = "Condition", ylab = "Length")
summary_data_Length <- by(bridge_data_subset$Length, bridge_data_subset$Condition, summary)
print(summary_data_Length)
# Create boxplot and summary for ImprovementCost
boxplot(ImprovementCost ~ Condition, data = bridge_data_subset,
main = "Boxplot of ImprovementCost Grouped by Condition",
xlab = "Condition", ylab = "ImprovementCost")
summary_data_ImprovementCost <- by(bridge_data_subset$ImprovementCost, bridge_data_subset$Condition, summary)
print(summary_data_ImprovementCost)
# Create boxplot and summary for Humidity
boxplot(Humidity ~ Condition, data = bridge_data_subset,
main = "Boxplot of Humidity Grouped by Condition",
xlab = "Condition", ylab = "Humidity")
summary_data_Humidity <- by(bridge_data_subset$Humidity, bridge_data_subset$Condition, summary)
print(summary_data_Humidity)
# Create boxplot and summary for Temperature
boxplot(Temperature ~ Condition, data = bridge_data_subset,
main = "Boxplot of Temperature Grouped by Condition",
xlab = "Condition", ylab = "Temperature")
summary_data_Temperature <- by(bridge_data_subset$Temperature, bridge_data_subset$Condition, summary)
print(summary_data_Temperature)
# Create boxplot and summary for DaysBelow0C
boxplot(DaysBelow0C ~ Condition, data = bridge_data_subset,
main = "Boxplot of DaysBelow0C Grouped by Condition",
xlab = "Condition", ylab = "DaysBelow0C")
summary_data_DaysBelow0C <- by(bridge_data_subset$DaysBelow0C, bridge_data_subset$Condition, summary)
print(summary_data_DaysBelow0C)
# Create boxplot and summary for Wetness
boxplot(Wetness ~ Condition, data = bridge_data_subset,
main = "Boxplot of Wetness Grouped by Condition",
xlab = "Condition", ylab = "Wetness")
summary_data_Wetness <- by(bridge_data_subset$Wetness, bridge_data_subset$Condition, summary)
print(summary_data_Wetness)
# Create boxplot and summary for Precipitation
boxplot(Precipitation ~ Condition, data = bridge_data_subset,
main = "Boxplot of Precipitation Grouped by Condition",
xlab = "Condition", ylab = "Precipitation")
summary_data_Precipitation <- by(bridge_data_subset$Precipitation, bridge_data_subset$Condition, summary)
print(summary_data_Precipitation)
# Load the corrplot library
library(corrplot)
# Compute the correlation matrix
cor_matrix <- cor(bridge_data_subset, use = "complete.obs")
# Set larger margins
par(mar = c(1, 1, 1, 1) + 0.05)
# Plot the correlation matrix displaying correlation coefficients
corrplot(cor_matrix, method = "number", tl.col = "black", tl.pos = "lt",
col = colorRampPalette(c("blue", "white", "red"))(10),
number.cex = 0.7)  # Adjust 'number.cex' to change the size of the numbers
library(dplyr)
# Example data
X <- bridge_data_subset[, !(names(bridge_data_subset) %in% c("Condition"))]  # Features
Y <- bridge_data_subset[, "Condition"]  # Target variable (already one-hot encoded)
# Unique classes
classes <- unique(Y)
set.seed(100)
# Function to split data for each class
split_data_by_class <- function(class_label) {
class_indices <- which(Y == class_label)
train_indices <- sample(class_indices, floor(0.8 * length(class_indices)))
test_indices <- setdiff(class_indices, train_indices)
return(list(train_indices = train_indices, test_indices = test_indices))
}
# Split data for each class
class_splits <- lapply(classes, split_data_by_class)
# Combine train and test indices for each class
train_indices <- unlist(lapply(class_splits, function(split) split$train_indices))
test_indices <- unlist(lapply(class_splits, function(split) split$test_indices))
# Split features and target based on indices
X_train <- X[train_indices, ]
Y_train <- Y[train_indices]
X_test <- X[test_indices, ]
Y_test <- Y[test_indices]
# Convert Y_train and Y_test to data frames with column names
Y_train <- data.frame(Condition = Y_train)
Y_test <- data.frame(Condition = Y_test)
# Save training and testing sets to CSV files
write.csv(X_train, file = "../Data/X_train.csv", row.names = FALSE)
write.csv(Y_train, file = "../Data/Y_train.csv", row.names = FALSE)
write.csv(X_test, file = "../Data/X_test.csv", row.names = FALSE)
write.csv(Y_test, file = "../Data/Y_test.csv", row.names = FALSE)
# Read data from CSV file
bridge_data <- read.csv("../Data/Last_Year_All_Field_Bridges.csv")
# Print the column names of the data frame
colnames(bridge_data)
# Select specific variables (columns) from the data frame
bridge_data_subset <- bridge_data[, c("Bridge.Age..yr.", "X29...Average.Daily.Traffic",
"X45...Number.of.Spans.in.Main.Unit", "X49...Structure.Length..ft..",
"X94...Bridge.Improvement.Cost", "Average.Relative.Humidity",
"Average.Temperature", "Number.of.Days.with.Temperature.Below.0.C",
"Time.of.Wetness", "Total.Precipitation", "CAT10...Bridge.Condition")]
# Rename the columns
colnames(bridge_data_subset) <- c("Age", "Traffic", "Spans", "Length",
"ImprovementCost", "Humidity", "Temperature",
"DaysBelow0C", "Wetness", "Precipitation", "Condition")
# Print the new column names to verify
print(colnames(bridge_data_subset))
# Check for missing data
colSums(is.na(bridge_data_subset))
# Remove rows with missing data (if any)
bridge_data_subset <- na.omit(bridge_data_subset)
print("Check data ")
colSums(is.na(bridge_data_subset))
# Check the unique values in the 'Condition' column
unique_conditions <- unique(bridge_data_subset$Condition)
print(unique_conditions)
# Encode the unique values as numbers starting from 0
bridge_data_subset$Condition <- as.numeric(factor(bridge_data_subset$Condition,
levels = unique_conditions,
labels = c(0, 1, 2)))
# Print the new encoded values
print(unique(bridge_data_subset$Condition))
summary(bridge_data_subset)
# Create boxplot and summary for Age
boxplot(Age ~ Condition, data = bridge_data_subset,
main = "Boxplot of Age Grouped by Condition",
xlab = "Condition", ylab = "Age")
summary_data_Age <- by(bridge_data_subset$Age, bridge_data_subset$Condition, summary)
print(summary_data_Age)
# Create boxplot and summary for Traffic
boxplot(Traffic ~ Condition, data = bridge_data_subset,
main = "Boxplot of Traffic Grouped by Condition",
xlab = "Condition", ylab = "Traffic")
summary_data_Traffic <- by(bridge_data_subset$Traffic, bridge_data_subset$Condition, summary)
print(summary_data_Traffic)
# Create boxplot and summary for Spans
boxplot(Spans ~ Condition, data = bridge_data_subset,
main = "Boxplot of Spans Grouped by Condition",
xlab = "Condition", ylab = "Spans")
summary_data_Spans <- by(bridge_data_subset$Spans, bridge_data_subset$Condition, summary)
print(summary_data_Spans)
# Create boxplot and summary for Length
boxplot(Length ~ Condition, data = bridge_data_subset,
main = "Boxplot of Length Grouped by Condition",
xlab = "Condition", ylab = "Length")
summary_data_Length <- by(bridge_data_subset$Length, bridge_data_subset$Condition, summary)
print(summary_data_Length)
# Create boxplot and summary for ImprovementCost
boxplot(ImprovementCost ~ Condition, data = bridge_data_subset,
main = "Boxplot of ImprovementCost Grouped by Condition",
xlab = "Condition", ylab = "ImprovementCost")
summary_data_ImprovementCost <- by(bridge_data_subset$ImprovementCost, bridge_data_subset$Condition, summary)
print(summary_data_ImprovementCost)
# Create boxplot and summary for Humidity
boxplot(Humidity ~ Condition, data = bridge_data_subset,
main = "Boxplot of Humidity Grouped by Condition",
xlab = "Condition", ylab = "Humidity")
summary_data_Humidity <- by(bridge_data_subset$Humidity, bridge_data_subset$Condition, summary)
print(summary_data_Humidity)
# Create boxplot and summary for Temperature
boxplot(Temperature ~ Condition, data = bridge_data_subset,
main = "Boxplot of Temperature Grouped by Condition",
xlab = "Condition", ylab = "Temperature")
summary_data_Temperature <- by(bridge_data_subset$Temperature, bridge_data_subset$Condition, summary)
print(summary_data_Temperature)
# Create boxplot and summary for DaysBelow0C
boxplot(DaysBelow0C ~ Condition, data = bridge_data_subset,
main = "Boxplot of DaysBelow0C Grouped by Condition",
xlab = "Condition", ylab = "DaysBelow0C")
summary_data_DaysBelow0C <- by(bridge_data_subset$DaysBelow0C, bridge_data_subset$Condition, summary)
print(summary_data_DaysBelow0C)
# Create boxplot and summary for Wetness
boxplot(Wetness ~ Condition, data = bridge_data_subset,
main = "Boxplot of Wetness Grouped by Condition",
xlab = "Condition", ylab = "Wetness")
summary_data_Wetness <- by(bridge_data_subset$Wetness, bridge_data_subset$Condition, summary)
print(summary_data_Wetness)
# Create boxplot and summary for Precipitation
boxplot(Precipitation ~ Condition, data = bridge_data_subset,
main = "Boxplot of Precipitation Grouped by Condition",
xlab = "Condition", ylab = "Precipitation")
summary_data_Precipitation <- by(bridge_data_subset$Precipitation, bridge_data_subset$Condition, summary)
print(summary_data_Precipitation)
# Load the corrplot library
library(corrplot)
# Compute the correlation matrix
cor_matrix <- cor(bridge_data_subset, use = "complete.obs")
# Set larger margins
par(mar = c(1, 1, 1, 1) + 0.05)
# Plot the correlation matrix displaying correlation coefficients
corrplot(cor_matrix, method = "number", tl.col = "black", tl.pos = "lt",
col = colorRampPalette(c("blue", "white", "red"))(10),
number.cex = 0.7)  # Adjust 'number.cex' to change the size of the numbers
library(dplyr)
# Example data
X <- bridge_data_subset[, !(names(bridge_data_subset) %in% c("Condition"))]  # Features
Y <- bridge_data_subset[, "Condition"]  # Target variable (already one-hot encoded)
# Unique classes
classes <- unique(Y)
set.seed(100)
# Function to split data for each class
split_data_by_class <- function(class_label) {
class_indices <- which(Y == class_label)
train_indices <- sample(class_indices, floor(0.8 * length(class_indices)))
test_indices <- setdiff(class_indices, train_indices)
return(list(train_indices = train_indices, test_indices = test_indices))
}
# Split data for each class
class_splits <- lapply(classes, split_data_by_class)
# Combine train and test indices for each class
train_indices <- unlist(lapply(class_splits, function(split) split$train_indices))
test_indices <- unlist(lapply(class_splits, function(split) split$test_indices))
# Split features and target based on indices
X_train <- X[train_indices, ]
Y_train <- Y[train_indices]
X_test <- X[test_indices, ]
Y_test <- Y[test_indices]
# Convert Y_train and Y_test to data frames with column names
Y_train <- data.frame(Condition = Y_train)
Y_test <- data.frame(Condition = Y_test)
# Save training and testing sets to CSV files
write.csv(X_train, file = "../Data/X_train.csv", row.names = FALSE)
write.csv(Y_train, file = "../Data/Y_train.csv", row.names = FALSE)
write.csv(X_test, file = "../Data/X_test.csv", row.names = FALSE)
write.csv(Y_test, file = "../Data/Y_test.csv", row.names = FALSE)
# Read file CSV
X_train <- read.csv("../Data/X_train.csv")
Y_train <- read.csv("../Data/Y_train.csv", header = TRUE)$Condition  # Ensure Y_train is read as a vector
X_test <- read.csv("../Data/X_test.csv")
Y_test <- read.csv("../Data/Y_test.csv", header = TRUE)$Condition  # Ensure Y_test is read as a vector
# Convert Y_train as_factor
if (!is.factor(Y_train)) {
Y_train <- as.factor(Y_train)
}
# Load torch library
library(torch)
library(torchvision)
if(cuda_is_available()){
device <- torch_device('cuda')
} else {
device <- torch_device('cpu')
}
# Convert data to tensors
X_train <- torch_tensor(as.matrix(X_train), dtype = torch_float())
X_test <- torch_tensor(as.matrix(X_test), dtype = torch_float())
Y_train <- torch_tensor(as.numeric(Y_train), dtype = torch_long())  # Adjust labels if needed
Y_test <- torch_tensor(as.numeric(Y_test), dtype = torch_long())    # Adjust labels if needed
# Define the MLP model with hidden layers
model <- nn_sequential(
nn_linear(in_features = ncol(X_train), out_features = 64),
nn_relu(),
nn_dropout(p=0.5),
nn_linear(in_features = 64, out_features = 32),
nn_relu(),
nn_dropout(p=0.5),
nn_linear(in_features = 32, out_features = 32),
nn_relu(),
nn_dropout(p=0.5),
nn_linear(in_features = 32, out_features = 16),
nn_relu(),
nn_dropout(p=0.5),
nn_linear(in_features = 16 ,out_features = 3),
)
# Define loss function and optimizer
criterion <- nn_cross_entropy_loss()
optimizer <- optim_adam(model$parameters, lr = 0.001)
num_epochs <- 1000
# Initialize vectors to store accuracies
accuracys_test <- c()
accuracys_train <- c()
# Initialize vectors to store loss history
loss_history_train <- c()
loss_history_test <- c()
for (epoch in 0:num_epochs) {
# Train for one epoch
optimizer$zero_grad()
Y_pred_train <- model(X_train)
Y_pred_test <- model(X_test)
loss_train <- criterion(Y_pred_train, Y_train)
loss_test <- criterion(Y_pred_test, Y_test)  # Calculate loss for test set
loss_train$backward()
optimizer$step()
# Append loss to loss_history vectors
loss_history_train <- c(loss_history_train, loss_train$item())
loss_history_test <- c(loss_history_test, loss_test$item())
if (epoch %% 5 == 0) {
predicted_train <- Y_pred_train$argmax(dim = 2)
accuracy_train <- (predicted_train == Y_train)$sum()$item() / Y_train$size()
predicted_labels <- Y_pred_test$argmax(dim = 2)
accuracy_test <- (predicted_labels == Y_test)$sum()$item() / Y_test$size()
cat("Epoch:", epoch, " Loss_Train:", loss_train$item(), " Loss_Test:", loss_test$item(), " Accuracy_Train: ", accuracy_train, " Accuracy_Test: ", accuracy_test, "\n")
# Append accuracies to respective vectors
accuracys_train <- c(accuracys_train, accuracy_train)
accuracys_test <- c(accuracys_test, accuracy_test)
}
}
# Evaluate the model
model$eval()
# Load the ggplot2 library
library(ggplot2)
# Create a dataframe to store the loss and accuracies
plot_data_loss <- data.frame(
epoch = seq(0, num_epochs, by = 5),
loss_train = rep(0, length.out = length(seq(0, num_epochs, by = 5))),
loss_test = rep(0, length.out = length(seq(0, num_epochs, by = 5)))
)
plot_data_accuracy <- data.frame(
epoch = seq(0, num_epochs, by = 5),
accuracy_train = rep(0, length.out = length(seq(0, num_epochs, by = 5))),
accuracy_test = rep(0, length.out = length(seq(0, num_epochs, by = 5)))
)
# Update plot_data_loss with loss values
for (i in 1:nrow(plot_data_loss)) {
plot_data_loss[i, "loss_train"] <- loss_history_train[i]  # Assuming loss_history_train is defined
plot_data_loss[i, "loss_test"] <- loss_history_test[i]  # Assuming loss_history_test is defined
}
# Update plot_data_accuracy with accuracy values
for (i in 1:nrow(plot_data_accuracy)) {
plot_data_accuracy[i, "accuracy_train"] <- accuracys_train[i]  # Assuming accuracys_train is defined
plot_data_accuracy[i, "accuracy_test"] <- accuracys_test[i]  # Assuming accuracys_test is defined
}
# Create the plot for loss
plot_loss <- ggplot(plot_data_loss, aes(x = epoch)) +
geom_line(aes(y = loss_train, color = "Training Loss")) +
geom_line(aes(y = loss_test, color = "Test Loss")) +
labs(title = "Loss Over Epochs",
x = "Epoch",
y = "Loss") +
scale_color_manual(values = c("Training Loss" = "blue", "Test Loss" = "red")) +
theme_minimal()
# Create the plot for accuracies
plot_accuracy <- ggplot(plot_data_accuracy, aes(x = epoch)) +
geom_line(aes(y = accuracy_train, color = "Training Accuracy")) +
geom_line(aes(y = accuracy_test, color = "Test Accuracy")) +
labs(title = "Accuracies Over Epochs",
x = "Epoch",
y = "Accuracy") +
scale_color_manual(values = c("Training Accuracy" = "green", "Test Accuracy" = "purple")) +
theme_minimal()
# Display the plots
print(plot_loss)
print(plot_accuracy)
# Read file CSV
X_train <- read.csv("../Data/X_train.csv")
Y_train <- read.csv("../Data/Y_train.csv", header = TRUE)$Condition  # Ensure Y_train is read as a vector
X_test <- read.csv("../Data/X_test.csv")
Y_test <- read.csv("../Data/Y_test.csv", header = TRUE)$Condition  # Ensure Y_test is read as a vector
# Convert Y_train as_factor
if (!is.factor(Y_train)) {
Y_train <- as.factor(Y_train)
}
library(randomForest)
# Kết hợp dữ liệu huấn luyện thành một dataframe
train_data <- data.frame(X_train, Y_train)
train_data$Y_train <- as.factor(train_data$Y_train)
colnames(train_data)[ncol(train_data)] <- "Y_train"
set.seed(100)
# Xây dựng mô hình Random Forest
model <- randomForest(Y_train ~ ., data = train_data, ntree = 250, mtry = 2, importance = TRUE)
# Dự đoán trên tập dữ liệu kiểm tra
predictions <- predict(model, X_test)
# Tạo bảng confusion matrix và tính độ chính xác
confusion_matrix <- table(Y_test, predictions)
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
# In kết quả
print(confusion_matrix)
print(paste("Accuracy:", round(accuracy, 4)))
# Xem độ quan trọng của các biến
print(importance(model))
varImpPlot(model)
# Read file CSV
X_train <- read.csv("../Data/X_train.csv")
Y_train <- read.csv("../Data/Y_train.csv", header = TRUE)$Condition  # Ensure Y_train is read as a vector
X_test <- read.csv("../Data/X_test.csv")
Y_test <- read.csv("../Data/Y_test.csv", header = TRUE)$Condition  # Ensure Y_test is read as a vector
# Convert Y_train as_factor
if (!is.factor(Y_train)) {
Y_train <- as.factor(Y_train)
}
library(xgboost)
set.seed(100)
# Chuyển đổi dữ liệu thành định dạng DMatrix của XGBoost
dtrain <- xgb.DMatrix(data = as.matrix(X_train), label = as.numeric(Y_train) - 1)
dtest <- xgb.DMatrix(data = as.matrix(X_test), label = as.numeric(Y_test) - 1)
# Đặt tham số cho mô hình XGBoost
params <- list(
objective = "multi:softprob",
num_class = length(unique(Y_train)),
eval_metric = "mlogloss",
eta = 0.1,  # learning rate
max_depth = 6,
subsample = 0.8,
colsample_bytree = 0.8
)
# Số vòng lặp (trees)
nrounds <- 100
# Huấn luyện mô hình
model <- xgb.train(
params = params,
data = dtrain,
nrounds = nrounds,
watchlist = list(train = dtrain, eval = dtest),
print_every_n = 10
)
# Dự đoán trên tập dữ liệu kiểm tra
pred_probs <- predict(model, as.matrix(X_test))
predictions <- max.col(matrix(pred_probs, ncol = length(unique(Y_train)), byrow = TRUE)) - 1
# Tạo bảng confusion matrix và tính độ chính xác
confusion_matrix <- table(Y_test, predictions)
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
# In kết quả
print(confusion_matrix)
print(paste("Accuracy:", round(accuracy, 4)))
# Xem độ quan trọng của các biến
importance_matrix <- xgb.importance(model = model)
print(importance_matrix)
xgb.plot.importance(importance_matrix)
# Read file CSV
X_train <- read.csv("../Data/X_train.csv")
Y_train <- read.csv("../Data/Y_train.csv", header = TRUE)$Condition  # Ensure Y_train is read as a vector
X_test <- read.csv("../Data/X_test.csv")
Y_test <- read.csv("../Data/Y_test.csv", header = TRUE)$Condition  # Ensure Y_test is read as a vector
# Convert Y_train as_factor
if (!is.factor(Y_train)) {
Y_train <- as.factor(Y_train)
}
# Load the nnet library for softmax regression
library(nnet)
set.seed(100)
# Ensure that "2" is one of the levels in Y_train$Condition
levels(Y_train) <- c(levels(Y_train), "2")
# Relevel Y_train$Condition
Y_train <- relevel(Y_train, ref = "2")
# Train softmax regression model
softmax_model <- multinom(Y_train ~ ., data = X_train)
summary(softmax_model)
z <- summary(softmax_model)$coefficients/summary(softmax_model)$standard.errors
z
# 2-tailed z test
p <- (1 - pnorm(abs(z), 0, 1)) * 2
p
p < 0.05
## extract the coefficients from the model and exponentiate
exp(coef(softmax_model))
head(pp <- fitted(softmax_model),7)
# Predictions on test set
predictions <- predict(softmax_model, newdata = X_test, "class")
# Calculate accuracy
accuracy <- mean(predictions == Y_test)
print(paste("Accuracy:", accuracy))
# Read file CSV
X_train <- read.csv("../Data/X_train.csv")
Y_train <- read.csv("../Data/Y_train.csv", header = TRUE)$Condition  # Ensure Y_train is read as a vector
X_test <- read.csv("../Data/X_test.csv")
Y_test <- read.csv("../Data/Y_test.csv", header = TRUE)$Condition  # Ensure Y_test is read as a vector
# Convert Y_train as_factor
if (!is.factor(Y_train)) {
Y_train <- as.factor(Y_train)
}
library(xgboost)
set.seed(100)
# Chuyển đổi dữ liệu thành định dạng DMatrix của XGBoost
dtrain <- xgb.DMatrix(data = as.matrix(X_train), label = as.numeric(Y_train) - 1)
dtest <- xgb.DMatrix(data = as.matrix(X_test), label = as.numeric(Y_test) - 1)
# Đặt tham số cho mô hình XGBoost
params <- list(
objective = "multi:softprob",
num_class = length(unique(Y_train)),
eval_metric = "mlogloss",
eta = 0.1,  # learning rate
max_depth = 6,
subsample = 0.8,
colsample_bytree = 0.8
)
# Số vòng lặp (trees)
nrounds <- 100
# Huấn luyện mô hình
model <- xgb.train(
params = params,
data = dtrain,
nrounds = nrounds,
watchlist = list(train = dtrain, eval = dtest),
print_every_n = 10
)
# Dự đoán trên tập dữ liệu kiểm tra
pred_probs <- predict(model, as.matrix(X_test))
predictions <- max.col(matrix(pred_probs, ncol = length(unique(Y_train)), byrow = TRUE)) - 1
# Tạo bảng confusion matrix và tính độ chính xác
confusion_matrix <- table(Y_test, predictions)
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
# In kết quả
print(confusion_matrix)
print(paste("Accuracy:", round(accuracy, 4)))
# Xem độ quan trọng của các biến
importance_matrix <- xgb.importance(model = model)
print(importance_matrix)
xgb.plot.importance(importance_matrix)
